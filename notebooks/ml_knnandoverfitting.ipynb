{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcoteran/ml/blob/master/notebooks/ml_knnandoverfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/ml/blob/master/notebooks/ml_knnandoverfitting.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesión 03: Clasificación no lineal, complejidad y sobreajuste\n",
    "## Guía Completa\n",
    "\n",
    "**Machine Learning**\n",
    "\n",
    "**Profesor:** Marco Terán  \n",
    "**Fecha:** 2025\n",
    "\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos primero unas librerías y funciones que vamos a usar a durante la sesión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Función para visualizar un conjunto de datos en 2D\n",
    "def plot_data(X, y):\n",
    "    y_unique = np.unique(y)\n",
    "    colors = pl.cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\n",
    "    for this_y, color in zip(y_unique, colors):\n",
    "        this_X = X[y == this_y]\n",
    "        pl.scatter(this_X[:, 0], this_X[:, 1],  c=color,\n",
    "                    alpha=0.5, edgecolor='k',\n",
    "                    label=\"Class %s\" % this_y)\n",
    "    pl.legend(loc=\"best\")\n",
    "    pl.title(\"Data\")\n",
    "    \n",
    "# Función para visualizar de la superficie de decisión de un clasificador\n",
    "def plot_decision_region(X, pred_fun):\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "    min_x = min_x - (max_x - min_x) * 0.05\n",
    "    max_x = max_x + (max_x - min_x) * 0.05\n",
    "    min_y = min_y - (max_y - min_y) * 0.05\n",
    "    max_y = max_y + (max_y - min_y) * 0.05\n",
    "    x_vals = np.linspace(min_x, max_x, 100)\n",
    "    y_vals = np.linspace(min_y, max_y, 100)\n",
    "    XX, YY = np.meshgrid(x_vals, y_vals)\n",
    "    grid_r, grid_c = XX.shape\n",
    "    ZZ = np.zeros((grid_r, grid_c))\n",
    "    for i in range(grid_r):\n",
    "        for j in range(grid_c):\n",
    "            ZZ[i, j] = pred_fun(XX[i, j], YY[i, j])\n",
    "    pl.contourf(XX, YY, ZZ, 100, cmap = pl.cm.coolwarm, vmin= -1, vmax=2)\n",
    "    pl.colorbar()\n",
    "    pl.xlabel(\"x\")\n",
    "    pl.ylabel(\"y\")\n",
    "    \n",
    "def gen_pred_fun(clf):\n",
    "    def pred_fun(x1, x2):\n",
    "        x = np.array([[x1, x2]])\n",
    "        return clf.predict(x)[0]\n",
    "    return pred_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de clasificación puede ser tan complejo como para aprenderse de memoria el conjunto de entrenamiento. Esta complejidad está determinada por los parámetros internos del modelo. A continuación, observaremos como se comporta esta complejidad usando un modelo clasificación no lineal como lo es  *k-vecinos más cercanos* (K-nearest neighbors en inglés)\n",
    "\n",
    "## Definición del conjunto de datos\n",
    "\n",
    "Vamos a trabajar con un conjunto de datos artificial. El conjunto es creado usando la funcionalidad `make_moons` de Scikit-Learn. `make_moons` permite introducir algo de ruido sobre las muestras creadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=600, noise=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('X ~ n_muestras x n_características:', X.shape)\n",
    "print('y ~ n_muestras:', y.shape)\n",
    "\n",
    "print('\\nPrimeras 5 muestras:\\n', X[:5, :])\n",
    "print('\\nPrimeras 5 etiquetas:', y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))  \n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que es dificil establecer una separación lineal como en regresión logística. Por lo tanto es necesario usar un modelo de clasificación no lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo: K-vecinos más cercanos\n",
    "\n",
    "La clasificación basada en vecinos es un tipo de aprendizaje basado en ejemplos. El modelo almacena los ejemplos vistos durante entrenamiento y clasifica un elemento no visto, usando una simple regla de votación por mayoría. Si se ubica un **punto** en el espacio de características, se le asigna como clase el valor de la clase que tenga la mayor cantidad de ejemplos en la vecindad del punto. Este ejemplo lo podemos ver ilustrado en la imagen:\n",
    "\n",
    "<img src=\"figures/knn.png\" width=\"50%\">\n",
    "\n",
    "Scikit-Learn provee una implementación del algoritmo conocida como `KNeighborsClassifier`. `KNeighborsClassifier` tiene un parámetro $n\\_neighbors$ o $k$, dónde $k$ es un entero definido por el usuario que determina cuantos vecinos evalua para determinar la clase de una instancia nunca antes vista. La elección de este parámetro es definida totalmente por la naturaleza de los datos.\n",
    "\n",
    "Observaremos que dependiendo del valor de vecinos más cercanos, conseguimos diferentes funciones de ajuste, unas más suaves que otras. Vamos a evaluar el efecto del parámetro $k$ en la complejidad del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a extraer un porcentaje al azar del conjunto de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(X), int(len(X)*.5), replace=False)\n",
    "X_reduced = X[idx]\n",
    "y_reduced = y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos un modelo `knn` llamando la función `fit()` sobre el conjunto de datos reducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_reduced, y_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))\n",
    "plot_decision_region(X_reduced, gen_pred_fun(knn))\n",
    "plot_data(X_reduced, y_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error: {}'.format(1 - knn.score(X_reduced, y_reduced)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Tiene sentido que el error sea del $0\\%$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora agreguemos los datos que descartamos anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_c = [i for i in range(len(X)) if i not in idx]\n",
    "X_complement = X[idx_c]\n",
    "y_complement = y[idx_c]\n",
    "\n",
    "pl.figure(figsize = (10, 6))   \n",
    "plot_decision_region(X_complement, gen_pred_fun(knn))\n",
    "plot_data(X_complement, y_complement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error: {}'.format(1 - knn.score(X_complement, y_complement)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que cuando el número de vecinos es $1$, el modelo se ajusta demasiado al ruído de los datos de entrada, por lo tanto sufre de **sobreajuste**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error de entrenamiento y generalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de aprendizaje de máquina tiene como objetivo principal hacer predicciones de manera acertada sobre ejemplos nunca antes vistos por el modelo. Esto se conoce como error de generalización. Para poder medir el error de generalización, dividimos el conjunto de datos en dos particiones: \n",
    "\n",
    "* **Entrenamiento**: Se usará para entrenar el modelo.\n",
    "* **Prueba**: Se usará para medir el error de generalización.\n",
    "\n",
    "En la siguiente imagen encontramos una ilustración de cómo se hace un particionamiento en entrenamiento y prueba.\n",
    "\n",
    "<img src=\"figures/train_test_split.svg\" width=\"50%\">\n",
    "\n",
    "Una de las prácticas recomendadas, es particionar los datos $70\\%$ para entrenamiento y $30\\%$ para prueba. Cuando el número de muestras es muy grande ($\\ge 70K$), podemos reducir el porcentaje de muestras para prueba, a $90-10\\%$. Sin embargo, deben hacerse unas aclaraciones sobre la generalización:\n",
    "\n",
    "* El conjunto de prueba debe ser una muestra representativa del conjunto de datos. El muestreo de ejemplos debe hacerse de forma independiente e idénticamente aleatoria de una distribución. Esto quiere decir, que el muestreo de un ejemplo no está influenciado por el muestreo de otro.\n",
    "* La distribución es estacionaria. Es decir no cambia a lo largo del conjunto de datos.\n",
    "* Los ejemplos son muestreados desde particiones de la misma distribución. Es decir, no se deben crear nuevas características en la partición de prueba.\n",
    "\n",
    "Adicionalmente, debemos tener en cuenta que se conserve la distribución de las etiquetas de los datos tanto en entrenamiento como en prueba (estratificación). En la siguiente sesión se va a estudiar en más detalle los efectos de hacer una partición estratificada. Scikit-Learn nos permite particionar un conjunto de datos en entrenamiento y prueba. A continuación vamos a dividir el conjunto en $70\\%$ para entrenamiento y $30\\%$ para prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parámetros:\n",
    "\n",
    "* `test_size`: Tamaño de la partición de prueba\n",
    "* `random_state`: Semilla del generador de números pseudoaleatorios. Este parámetro garantiza reproducibilidad del particionamiento.\n",
    "* `stratify`: Si se estratifican los datos con respecto a `y`\n",
    "\n",
    "Vamos a verificar el número de muestras de ambas particiones y la distribución de clases de cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Número de muestras en entrenamiento: {}'.format(X_train.shape[0]))\n",
    "print('Número de muestras en prueba: {}'.format(X_test.shape[0]))\n",
    "print('Número de características: {}'.format(X_train.shape[1]))\n",
    "\n",
    "print('Distribución de clases en entrenamiento: {}'.format(np.bincount(y_train)))\n",
    "print('Distribución de clases en prueba: {}'.format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la partición recién creada y analizamos un modelo entrenado con $k = 200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=200)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6)) \n",
    "plot_decision_region(X_train, gen_pred_fun(knn))\n",
    "plot_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error en entrenamiento: {}'.format(1-knn.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que el error en entrenamiento es del $14\\%$. Además se evidencia que el modelo entrenado es ahora demasiado **simple** y no se puede ajustar la estructura de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora medimos el error de generalización del modelo entrenado y visualizamos la clasificación de los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))   \n",
    "plot_decision_region(X_test, gen_pred_fun(knn))\n",
    "plot_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error de generalización: {}'.format(1 - knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que cuando aumentamos el número de vecinos, nuestro modelo sufre de **subajuste**. La superficie de decisión se suaviza, pero no logra captar los detalles de los datos. Mientras el error de entrenamiento se acerca a $14\\%$, el error de generalización se acerca a $10\\%$.\n",
    "\n",
    "**¿Cómo estimar un buen número de $k$-vecinos más cercanos de manera que el modelo no sobreajuste ni subajuste los datos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de la complejidad\n",
    "\n",
    "Un modelo de aprendizaje de máquina puede ser tan complejo como para recordar las particularidades y el ruido del conjunto de entrenamiento (**sobreajuste**), así como puede ser demasiado flexible para no modelar la variabilidad de los datos (**subajuste**). El modelo debe garantizar un compromiso entre el sobreajuste y el subajuste, lo cual se logra evaluando la complejidad del modelo. Una forma de evaluar la complejidad es analizar el error de entrenamiento y generalización para diferentes modelos que varían en su complejidad. En el caso de `KNearestNeighbor`, la complejidad está determinada por el número de vecinos $k$. **Entre menor sea el número de vecinos, más complejo es el modelo.**\n",
    "\n",
    "A continuación exploramos un conjunto de valores $k$, con el objetivo de encontrar aquel modelo con el mejor compromiso entre error de entrenamiento y error de generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = list(range(1, 20))\n",
    "print(k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar un nuevo conjunto de datos con $1000$ muestras y haremos una partición $70-30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.4, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el error de entrenamiento y generalización con respecto al aumento de complejidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = []\n",
    "generalization_error = []\n",
    "\n",
    "for nn in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=nn)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_error.append(1 - knn.score(X_train, y_train))\n",
    "    generalization_error.append(1 - knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos ambas curvas de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))\n",
    "\n",
    "pl.plot(k_values, train_error, label=\"Entrenamiento\")\n",
    "pl.plot(k_values, generalization_error, label=\"Generalización\")\n",
    "pl.xticks(k_values)\n",
    "pl.xlabel(\"k-vecinos\")\n",
    "pl.ylabel(\"Error\")\n",
    "#pl.gca().invert_xaxis()\n",
    "pl.arrow(13, 0.16, 0, -0.01, head_width=0.2, head_length=0.01, fc='k', ec='k')\n",
    "pl.text(15, 0.165, 'Punto de balance')\n",
    "pl.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que el error de entrenamiento y generalización tiene su punto de balance mínimo con $k=13$. Observamos tambien que cuando el modelo es demasiado complejo ($k=1$), el error de generalización sube, así como el de entrenamiento cae a $0\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo con el conjunto de datos **Wine**.\n",
    "\n",
    "# Realice:\n",
    "\n",
    "* Cargue el conjunto de datos **Wine**.\n",
    "* Genere una partición estratificada 70-30 sobre el conjunto de datos.\n",
    "* Genere una gráfica de barras en Pandas que verifique que las particiones de entrenamiento y prueba tengan la misma distribución de clases.\n",
    "\n",
    "## Explorando la complejidad usando `KNearestNeighbor`\n",
    "* Entrene un modelo `KNearestNeighbor`. Use los siguientes valores para evaluar la complejidad:\n",
    "    * $[1, 2, 3, \\dots, 20]$\n",
    "    * Grafique los errores de entrenamiento y generalización conforme a la complejidad el modelo aumenta.\n",
    "* Construya un conjunto de datos usando las características `Proline` contra `Flavonoids`. \n",
    "    * Use la partición 70-30 definida al inicio\n",
    "    * Usando `KNearestNeighbor`, determine el número subóptimo de k-vecinos usando evaluación de la complejidad. Use los mismos valores de $k$: $[1, 2, 3, \\dots, 20]$\n",
    "    * Grafique la superficie de decisión contra los ejemplos de test.\n",
    "    * Reporte accuracy, el error de clasificación, la precisión macro, el recall macro y el F1 score macro sobre el **conjunto de prueba**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
