{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcoteran/ml/blob/master/notebooks/ml_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/ml/blob/master/notebooks/ml_regression.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesi√≥n 02: Regresi√≥n Lineal y Log√≠stica\n",
    "## Gu√≠a Completa\n",
    "\n",
    "**Machine Learning**\n",
    "\n",
    "**Profesor:** Marco Ter√°n  \n",
    "**Fecha:** 2025\n",
    "\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Tabla de Contenidos\n",
    "\n",
    "1. **[Introducci√≥n y Setup](#1-introducci√≥n-y-setup)**\n",
    "2. **[Regresi√≥n Lineal Simple](#2-regresi√≥n-lineal-simple)**\n",
    "3. **[Regresi√≥n Lineal M√∫ltiple](#3-regresi√≥n-lineal-m√∫ltiple)**\n",
    "4. **[Gradient Descent desde Cero](#4-gradient-descent-desde-cero)**\n",
    "6. **[Regularizaci√≥n](#6-regularizaci√≥n)**\n",
    "7. **[Regresi√≥n Log√≠stica](#7-regresi√≥n-log√≠stica)**\n",
    "8. **[M√©tricas de Clasificaci√≥n](#8-m√©tricas-de-clasificaci√≥n)**\n",
    "9. **[Casos Pr√°cticos Reales](#9-casos-pr√°cticos-reales)**\n",
    "10. **[Proyecto Final Integrador](#10-proyecto-final-integrador)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducci√≥n y Setup\n",
    "\n",
    "### üìñ Teor√≠a: ¬øQu√© es la Regresi√≥n?\n",
    "\n",
    "La **regresi√≥n** es una t√©cnica de aprendizaje supervisado que busca modelar la relaci√≥n entre variables. Existen dos tipos principales:\n",
    "\n",
    "- **Regresi√≥n Lineal**: Predice valores continuos (precios, temperaturas, ventas)\n",
    "- **Regresi√≥n Log√≠stica**: Predice probabilidades/categor√≠as (spam/no spam, aprobado/rechazado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Instalaci√≥n y Configuraci√≥n del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar todas las librer√≠as necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as de Machine Learning\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, ElasticNet,\n",
    "                                  LogisticRegression, RidgeCV, LassoCV)\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\n",
    "                           accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           confusion_matrix, roc_curve, auc, roc_auc_score, classification_report)\n",
    "from sklearn.datasets import load_diabetes, load_breast_cancer, load_iris\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuraci√≥n de pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as cargadas exitosamente\")\n",
    "print(f\"üìä Versiones:\")\n",
    "print(f\"  ‚Ä¢ NumPy: {np.__version__}\")\n",
    "print(f\"  ‚Ä¢ Pandas: {pd.__version__}\")\n",
    "print(f\"  ‚Ä¢ Scikit-learn: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also requires Scikit-Learn ‚â• 1.0.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé® Funciones de Utilidad para Visualizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paleta de colores consistente\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB',\n",
    "    'secondary': '#A23B72',\n",
    "    'accent': '#F18F01',\n",
    "    'success': '#73AB84',\n",
    "    'warning': '#C73E1D',\n",
    "    'dark': '#2D3142',\n",
    "    'light': '#F0F0F0'\n",
    "}\n",
    "\n",
    "def plot_style(ax, title=\"\", xlabel=\"\", ylabel=\"\"):\n",
    "    \"\"\"Aplicar estilo consistente a los gr√°ficos\"\"\"\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"Imprimir t√≠tulo de secci√≥n con estilo\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Regresi√≥n Lineal Simple\n",
    "\n",
    "### üìñ Teor√≠a: Modelo Lineal Simple\n",
    "\n",
    "El modelo de regresi√≥n lineal simple busca una relaci√≥n lineal entre una variable independiente $x$ y una variable dependiente $y$:\n",
    "\n",
    "$$y = \\theta_0 + \\theta_1 x + \\epsilon$$\n",
    "\n",
    "Donde:\n",
    "- $\\theta_0$: Intercepto (bias)\n",
    "- $\\theta_1$: Pendiente (peso)\n",
    "- $\\epsilon$: Error aleatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª Implementaci√≥n desde Cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"2.1 REGRESI√ìN LINEAL SIMPLE - IMPLEMENTACI√ìN MANUAL\")\n",
    "\n",
    "class SimpleLinearRegression:\n",
    "    \"\"\"\n",
    "    Implementaci√≥n desde cero de Regresi√≥n Lineal Simple\n",
    "    usando la ecuaci√≥n normal (m√©todo de m√≠nimos cuadrados)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta_0 = None  # Intercepto\n",
    "        self.theta_1 = None  # Pendiente\n",
    "        self.r_squared = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Ajustar el modelo usando la ecuaci√≥n normal\n",
    "        Œ∏‚ÇÅ = Œ£((x - xÃÑ)(y - »≥)) / Œ£((x - xÃÑ)¬≤)\n",
    "        Œ∏‚ÇÄ = »≥ - Œ∏‚ÇÅxÃÑ\n",
    "        \"\"\"\n",
    "        X = np.array(X).flatten()\n",
    "        y = np.array(y).flatten()\n",
    "        \n",
    "        # Calcular medias\n",
    "        x_mean = np.mean(X)\n",
    "        y_mean = np.mean(y)\n",
    "        \n",
    "        # Calcular pendiente\n",
    "        numerator = np.sum((X - x_mean) * (y - y_mean))\n",
    "        denominator = np.sum((X - x_mean) ** 2)\n",
    "        self.theta_1 = numerator / denominator\n",
    "        \n",
    "        # Calcular intercepto\n",
    "        self.theta_0 = y_mean - self.theta_1 * x_mean\n",
    "        \n",
    "        # Calcular R¬≤\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - y_mean) ** 2)\n",
    "        self.r_squared = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Realizar predicciones\"\"\"\n",
    "        X = np.array(X).flatten()\n",
    "        return self.theta_0 + self.theta_1 * X\n",
    "    \n",
    "    def get_equation(self):\n",
    "        \"\"\"Obtener ecuaci√≥n del modelo\"\"\"\n",
    "        return f\"y = {self.theta_0:.2f} + {self.theta_1:.2f}x\"\n",
    "    \n",
    "    def plot_fit(self, X, y, title=\"Regresi√≥n Lineal Simple\"):\n",
    "        \"\"\"Visualizar el ajuste del modelo\"\"\"\n",
    "        X = np.array(X).flatten()\n",
    "        y = np.array(y).flatten()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Gr√°fico de ajuste\n",
    "        axes[0].scatter(X, y, alpha=0.6, s=50, color=COLORS['accent'], \n",
    "                       edgecolors='black', linewidth=0.5, label='Datos reales')\n",
    "        \n",
    "        # L√≠nea de regresi√≥n\n",
    "        X_line = np.linspace(X.min(), X.max(), 100)\n",
    "        y_pred_line = self.predict(X_line)\n",
    "        axes[0].plot(X_line, y_pred_line, color=COLORS['primary'], \n",
    "                    linewidth=2, label=self.get_equation())\n",
    "        \n",
    "        # Residuos\n",
    "        y_pred = self.predict(X)\n",
    "        for xi, yi, yi_pred in zip(X, y, y_pred):\n",
    "            axes[0].plot([xi, xi], [yi, yi_pred], 'k--', alpha=0.3, linewidth=0.8)\n",
    "        \n",
    "        plot_style(axes[0], title, \"X\", \"Y\")\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # A√±adir m√©tricas\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        mae = np.mean(np.abs(y - y_pred))\n",
    "        axes[0].text(0.05, 0.95, f'R¬≤ = {self.r_squared:.4f}\\nMSE = {mse:.4f}\\nMAE = {mae:.4f}',\n",
    "                    transform=axes[0].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        # Gr√°fico de residuos\n",
    "        residuals = y - y_pred\n",
    "        axes[1].scatter(y_pred, residuals, alpha=0.6, s=50, \n",
    "                       color=COLORS['secondary'], edgecolors='black', linewidth=0.5)\n",
    "        axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "        axes[1].fill_between([y_pred.min(), y_pred.max()], \n",
    "                            [-2*residuals.std(), -2*residuals.std()],\n",
    "                            [2*residuals.std(), 2*residuals.std()],\n",
    "                            alpha=0.2, color='gray', label='¬±2œÉ')\n",
    "        \n",
    "        plot_style(axes[1], \"An√°lisis de Residuos\", \"Valores Predichos\", \"Residuos\")\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo con datos sint√©ticos\n",
    "print(\"üìä Generando datos sint√©ticos para demostraci√≥n...\")\n",
    "np.random.seed(42)\n",
    "X_simple = np.random.uniform(0, 10, 100)\n",
    "y_simple = 2.5 * X_simple + 10 + np.random.normal(0, 2, 100)\n",
    "\n",
    "# Entrenar modelo\n",
    "model_simple = SimpleLinearRegression()\n",
    "model_simple.fit(X_simple, y_simple)\n",
    "\n",
    "print(f\"‚úÖ Modelo entrenado:\")\n",
    "print(f\"   Ecuaci√≥n: {model_simple.get_equation()}\")\n",
    "print(f\"   R¬≤: {model_simple.r_squared:.4f}\")\n",
    "\n",
    "# Visualizar\n",
    "model_simple.plot_fit(X_simple, y_simple, \"Regresi√≥n Lineal Simple - Datos Sint√©ticos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Comparaci√≥n con Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"2.2 COMPARACI√ìN CON SCIKIT-LEARN\")\n",
    "\n",
    "# Usar scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Preparar datos (sklearn espera matrices 2D)\n",
    "X_sklearn = X_simple.reshape(-1, 1)\n",
    "\n",
    "# Entrenar modelo\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X_sklearn, y_simple)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"üìä Comparaci√≥n de resultados:\")\n",
    "print(f\"{'M√©todo':<20} {'Œ∏‚ÇÄ (Intercepto)':<15} {'Œ∏‚ÇÅ (Pendiente)':<15} {'R¬≤':<10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Implementaci√≥n':<20} {model_simple.theta_0:<15.4f} {model_simple.theta_1:<15.4f} {model_simple.r_squared:<10.4f}\")\n",
    "print(f\"{'Scikit-learn':<20} {model_sklearn.intercept_:<15.4f} {model_sklearn.coef_[0]:<15.4f} {model_sklearn.score(X_sklearn, y_simple):<10.4f}\")\n",
    "print(\"\\n‚úÖ Los resultados son id√©nticos, validando nuestra implementaci√≥n!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio Interactivo: Efecto de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"2.3 EFECTO DE OUTLIERS EN REGRESI√ìN LINEAL\")\n",
    "\n",
    "def demonstrate_outlier_effect():\n",
    "    \"\"\"Demostrar c√≥mo los outliers afectan la regresi√≥n lineal\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Datos originales\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(0, 10, 50)\n",
    "    y = 2 * X + 5 + np.random.normal(0, 1, 50)\n",
    "    \n",
    "    # Sin outliers\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X.reshape(-1, 1), y)\n",
    "    axes[0].scatter(X, y, alpha=0.6, color=COLORS['accent'])\n",
    "    axes[0].plot(X, model1.predict(X.reshape(-1, 1)), \n",
    "                color=COLORS['primary'], linewidth=2)\n",
    "    plot_style(axes[0], \"Sin Outliers\", \"X\", \"Y\")\n",
    "    axes[0].text(0.05, 0.95, f'R¬≤ = {model1.score(X.reshape(-1, 1), y):.3f}',\n",
    "                transform=axes[0].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "    \n",
    "    # Con 1 outlier\n",
    "    X_out1 = np.append(X, [9.5])\n",
    "    y_out1 = np.append(y, [50])  # Outlier\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X_out1.reshape(-1, 1), y_out1)\n",
    "    axes[1].scatter(X, y, alpha=0.6, color=COLORS['accent'])\n",
    "    axes[1].scatter([9.5], [50], color='red', s=100, marker='x', linewidth=3)\n",
    "    axes[1].plot(X_out1, model2.predict(X_out1.reshape(-1, 1)), \n",
    "                color=COLORS['primary'], linewidth=2)\n",
    "    plot_style(axes[1], \"Con 1 Outlier\", \"X\", \"Y\")\n",
    "    axes[1].text(0.05, 0.95, f'R¬≤ = {model2.score(X_out1.reshape(-1, 1), y_out1):.3f}',\n",
    "                transform=axes[1].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='yellow'))\n",
    "    \n",
    "    # Con 3 outliers\n",
    "    X_out3 = np.append(X, [9.5, 8.5, 7.5])\n",
    "    y_out3 = np.append(y, [50, 45, 40])  # Outliers\n",
    "    model3 = LinearRegression()\n",
    "    model3.fit(X_out3.reshape(-1, 1), y_out3)\n",
    "    axes[2].scatter(X, y, alpha=0.6, color=COLORS['accent'])\n",
    "    axes[2].scatter([9.5, 8.5, 7.5], [50, 45, 40], color='red', \n",
    "                   s=100, marker='x', linewidth=3)\n",
    "    axes[2].plot(np.sort(X_out3), model3.predict(np.sort(X_out3).reshape(-1, 1)), \n",
    "                color=COLORS['primary'], linewidth=2)\n",
    "    plot_style(axes[2], \"Con 3 Outliers\", \"X\", \"Y\")\n",
    "    axes[2].text(0.05, 0.95, f'R¬≤ = {model3.score(X_out3.reshape(-1, 1), y_out3):.3f}',\n",
    "                transform=axes[2].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
    "    \n",
    "    plt.suptitle(\"Impacto de Outliers en Regresi√≥n Lineal\", fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Observaciones:\")\n",
    "    print(\"  ‚Ä¢ Los outliers pueden cambiar dram√°ticamente la l√≠nea de regresi√≥n\")\n",
    "    print(\"  ‚Ä¢ El R¬≤ disminuye significativamente con outliers\")\n",
    "    print(\"  ‚Ä¢ Considerar t√©cnicas robustas (RANSAC, Huber) para datos con outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrate_outlier_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Regresi√≥n Lineal M√∫ltiple\n",
    "\n",
    "### üìñ Teor√≠a: M√∫ltiples Variables Predictoras\n",
    "\n",
    "Cuando tenemos m√∫ltiples caracter√≠sticas, el modelo se extiende a:\n",
    "\n",
    "$$y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$$\n",
    "\n",
    "En forma matricial: $y = X\\theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª Dataset Real: Predicci√≥n de Precios de Casas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"3.1 REGRESI√ìN M√öLTIPLE - DATASET CALIFORNIA HOUSING\")\n",
    "\n",
    "# Cargar dataset de California Housing\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Cargar datos\n",
    "california = fetch_california_housing()\n",
    "X_calif = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y_calif = california.target\n",
    "\n",
    "print(\"üìä Informaci√≥n del Dataset:\")\n",
    "print(f\"  ‚Ä¢ N√∫mero de muestras: {X_calif.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ N√∫mero de caracter√≠sticas: {X_calif.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Variable objetivo: Precio medio de casas (en cientos de miles de d√≥lares)\")\n",
    "print(\"\\nüìã Caracter√≠sticas disponibles:\")\n",
    "for i, col in enumerate(X_calif.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\nüìà Estad√≠sticas Descriptivas:\")\n",
    "display(X_calif.describe())\n",
    "\n",
    "# Visualizar distribuciones\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(X_calif.columns):\n",
    "    axes[idx].hist(X_calif[col], bins=30, color=COLORS['accent'], \n",
    "                   alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(col, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Frecuencia')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "axes[8].hist(y_calif, bins=30, color=COLORS['success'], \n",
    "             alpha=0.7, edgecolor='black')\n",
    "axes[8].set_title('Precio (Target)', fontweight='bold')\n",
    "axes[8].set_ylabel('Frecuencia')\n",
    "axes[8].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Distribuci√≥n de Variables - California Housing\", \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä An√°lisis de Correlaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"3.2 AN√ÅLISIS DE CORRELACI√ìN\")\n",
    "\n",
    "# Crear DataFrame completo\n",
    "df_calif = X_calif.copy()\n",
    "df_calif['Price'] = y_calif\n",
    "\n",
    "# Matriz de correlaci√≥n\n",
    "correlation_matrix = df_calif.corr()\n",
    "\n",
    "# Visualizaci√≥n de correlaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True,\n",
    "            linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
    "axes[0].set_title('Matriz de Correlaci√≥n', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Correlaci√≥n con variable objetivo\n",
    "target_corr = correlation_matrix['Price'].sort_values(ascending=False)[1:]\n",
    "colors = [COLORS['success'] if x > 0 else COLORS['warning'] for x in target_corr]\n",
    "bars = axes[1].barh(target_corr.index, target_corr.values, color=colors)\n",
    "axes[1].set_xlabel('Correlaci√≥n con Precio')\n",
    "axes[1].set_title('Correlaci√≥n de Features con Precio', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# A√±adir valores\n",
    "for bar, value in zip(bars, target_corr.values):\n",
    "    axes[1].text(value + 0.01 if value > 0 else value - 0.01, \n",
    "                bar.get_y() + bar.get_height()/2, \n",
    "                f'{value:.3f}', va='center', ha='left' if value > 0 else 'right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Observaciones clave:\")\n",
    "print(f\"  ‚Ä¢ Mayor correlaci√≥n positiva: {target_corr.index[0]} ({target_corr.values[0]:.3f})\")\n",
    "print(f\"  ‚Ä¢ Mayor correlaci√≥n negativa: {target_corr.index[-1]} ({target_corr.values[-1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Entrenamiento del Modelo M√∫ltiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"3.3 ENTRENAMIENTO Y EVALUACI√ìN\")\n",
    "\n",
    "# Preparar datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_calif, y_calif, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Escalar caracter√≠sticas (importante para regresi√≥n m√∫ltiple)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar modelo\n",
    "model_multiple = LinearRegression()\n",
    "model_multiple.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred = model_multiple.predict(X_train_scaled)\n",
    "y_test_pred = model_multiple.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    \"\"\"Calcular y mostrar m√©tricas de evaluaci√≥n\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä M√©tricas para {dataset_name}:\")\n",
    "    print(f\"  ‚Ä¢ MSE:  {mse:.4f}\")\n",
    "    print(f\"  ‚Ä¢ RMSE: {rmse:.4f}\")\n",
    "    print(f\"  ‚Ä¢ MAE:  {mae:.4f}\")\n",
    "    print(f\"  ‚Ä¢ R¬≤:   {r2:.4f}\")\n",
    "    \n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, \"Entrenamiento\")\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, \"Prueba\")\n",
    "\n",
    "# Visualizaci√≥n de predicciones\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Gr√°fico de predicciones vs reales\n",
    "for ax, y_true, y_pred, title, metrics in [\n",
    "    (axes[0], y_train, y_train_pred, \"Conjunto de Entrenamiento\", train_metrics),\n",
    "    (axes[1], y_test, y_test_pred, \"Conjunto de Prueba\", test_metrics)\n",
    "]:\n",
    "    ax.scatter(y_true, y_pred, alpha=0.5, s=20, color=COLORS['accent'])\n",
    "    ax.plot([y_true.min(), y_true.max()], \n",
    "            [y_true.min(), y_true.max()], \n",
    "            'r--', linewidth=2, label='Predicci√≥n Perfecta')\n",
    "    \n",
    "    # A√±adir banda de error\n",
    "    error_band = 0.5  # ¬±0.5 para visualizaci√≥n\n",
    "    ax.fill_between([y_true.min(), y_true.max()],\n",
    "                    [y_true.min() - error_band, y_true.max() - error_band],\n",
    "                    [y_true.min() + error_band, y_true.max() + error_band],\n",
    "                    alpha=0.2, color='gray', label=f'¬±{error_band}')\n",
    "    \n",
    "    plot_style(ax, title, \"Valor Real\", \"Valor Predicho\")\n",
    "    ax.legend()\n",
    "    \n",
    "    # A√±adir m√©tricas\n",
    "    metrics_text = f\"R¬≤ = {metrics['r2']:.3f}\\nRMSE = {metrics['rmse']:.3f}\"\n",
    "    ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes,\n",
    "           verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.suptitle(\"Evaluaci√≥n del Modelo de Regresi√≥n M√∫ltiple\", \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Importancia de Caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"3.4 IMPORTANCIA DE CARACTER√çSTICAS\")\n",
    "\n",
    "# Obtener coeficientes\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_calif.columns,\n",
    "    'Coefficient': model_multiple.coef_,\n",
    "    'Abs_Coefficient': np.abs(model_multiple.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"üìä Coeficientes del Modelo (ordenados por importancia absoluta):\")\n",
    "display(feature_importance)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Coeficientes\n",
    "colors = [COLORS['success'] if x > 0 else COLORS['warning'] \n",
    "          for x in feature_importance['Coefficient']]\n",
    "bars = axes[0].barh(feature_importance['Feature'], \n",
    "                   feature_importance['Coefficient'], \n",
    "                   color=colors)\n",
    "axes[0].set_xlabel('Coeficiente')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "plot_style(axes[0], \"Coeficientes del Modelo\", \"Valor del Coeficiente\", \"\")\n",
    "\n",
    "# Importancia absoluta\n",
    "axes[1].barh(feature_importance['Feature'], \n",
    "            feature_importance['Abs_Coefficient'],\n",
    "            color=COLORS['primary'])\n",
    "axes[1].set_xlabel('|Coeficiente|')\n",
    "plot_style(axes[1], \"Importancia Absoluta\", \"Magnitud\", \"\")\n",
    "\n",
    "plt.suptitle(\"An√°lisis de Importancia de Caracter√≠sticas\", \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretaci√≥n:\")\n",
    "print(f\"  ‚Ä¢ Caracter√≠stica m√°s importante: {feature_importance.iloc[0]['Feature']}\")\n",
    "print(f\"    Un aumento de 1 std en {feature_importance.iloc[0]['Feature']} \")\n",
    "print(f\"    cambia el precio en {feature_importance.iloc[0]['Coefficient']:.3f} unidades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Gradient Descent desde Cero\n",
    "\n",
    "### üìñ Teor√≠a: Optimizaci√≥n Iterativa\n",
    "\n",
    "El Gradient Descent encuentra los par√°metros √≥ptimos minimizando iterativamente la funci√≥n de costo:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª Implementaci√≥n Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gr√°fica en 3D de nuestra funci√≥n de coste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm # Para manejar colores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "def f(x,y):\n",
    "  return x**2 + y**2;\n",
    "\n",
    "res = 100\n",
    "\n",
    "X = np.linspace(-4, 4, res)\n",
    "Y = np.linspace(-4, 4, res)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "Z = f(X,Y) \n",
    "\n",
    "# Gr√°ficar la superficie\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.cool,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "fig.colorbar(surf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_map = np.linspace(np.min(Z), np.max(Z),res) \n",
    "plt.contourf(X, Y, Z, levels=level_map,cmap=cm.cool)\n",
    "plt.colorbar()\n",
    "plt.title('Descenso del gradiente')\n",
    "\n",
    "def derivate(_p,p):\n",
    "  return  (f(_p[0],_p[1]) - f(p[0],p[1])) / h\n",
    "\n",
    "p = np.random.rand(2) * 8 - 4 # generar dos valores aleatorios\n",
    "\n",
    "plt.plot(p[0],p[1],'o', c='k')\n",
    "\n",
    "lr = 0.01\n",
    "h = 0.01\n",
    "\n",
    "grad = np.zeros(2)\n",
    "\n",
    "for i in range(10000):\n",
    "  for idx, val in enumerate(p): \n",
    "    _p = np.copy(p)\n",
    "\n",
    "    _p[idx] = _p[idx] + h;\n",
    "\n",
    "    dp = derivate(_p,p) \n",
    "\n",
    "    grad[idx] = dp\n",
    "\n",
    "  p = p - lr * grad\n",
    "\n",
    "  if(i % 10 == 0):\n",
    "    plt.plot(p[0],p[1],'o', c='r')\n",
    "\n",
    "plt.plot(p[0],p[1],'o', c='w')\n",
    "plt.show()\n",
    "\n",
    "print(\"El punto m√≠nimo se encuentra en: \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"4.1 GRADIENT DESCENT - IMPLEMENTACI√ìN DESDE CERO\")\n",
    "\n",
    "class GradientDescentRegression:\n",
    "    \"\"\"\n",
    "    Implementaci√≥n de Regresi√≥n Lineal usando Gradient Descent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.verbose = verbose\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"Agregar columna de unos para el intercepto\"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.c_[intercept, X]\n",
    "    \n",
    "    def _compute_cost(self, X, y, theta):\n",
    "        \"\"\"Calcular funci√≥n de costo MSE\"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X.dot(theta)\n",
    "        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def _gradient_descent(self, X, y):\n",
    "        \"\"\"Algoritmo de gradient descent\"\"\"\n",
    "        m = len(y)\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            # Predicciones actuales\n",
    "            predictions = X.dot(self.theta)\n",
    "            \n",
    "            # Calcular errores\n",
    "            errors = predictions - y\n",
    "            \n",
    "            # Calcular gradientes\n",
    "            gradients = (1 / m) * X.T.dot(errors)\n",
    "            \n",
    "            # Actualizar par√°metros\n",
    "            self.theta = self.theta - self.learning_rate * gradients\n",
    "            \n",
    "            # Guardar costo\n",
    "            cost = self._compute_cost(X, y, self.theta)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Imprimir progreso\n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f\"  Iteraci√≥n {i}: Costo = {cost:.6f}\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrenar el modelo\"\"\"\n",
    "        # Preparar datos\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        X = self._add_intercept(X)\n",
    "        \n",
    "        # Inicializar par√°metros\n",
    "        self.theta = np.zeros((X.shape[1], 1))\n",
    "        \n",
    "        # Ejecutar gradient descent\n",
    "        self._gradient_descent(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Realizar predicciones\"\"\"\n",
    "        X = np.array(X)\n",
    "        X = self._add_intercept(X)\n",
    "        return X.dot(self.theta).flatten()\n",
    "    \n",
    "    def plot_convergence(self):\n",
    "        \"\"\"Visualizar convergencia del algoritmo\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Convergencia completa\n",
    "        axes[0].plot(self.cost_history, color=COLORS['primary'], linewidth=2)\n",
    "        axes[0].set_xlabel('Iteraci√≥n')\n",
    "        axes[0].set_ylabel('Costo J(Œ∏)')\n",
    "        plot_style(axes[0], \"Convergencia del Gradient Descent\", \n",
    "                  \"Iteraci√≥n\", \"Costo\")\n",
    "        \n",
    "        # Primeras 100 iteraciones\n",
    "        axes[1].plot(self.cost_history[:100], color=COLORS['accent'], linewidth=2)\n",
    "        axes[1].set_xlabel('Iteraci√≥n')\n",
    "        axes[1].set_ylabel('Costo J(Œ∏)')\n",
    "        plot_style(axes[1], \"Primeras 100 Iteraciones\", \n",
    "                  \"Iteraci√≥n\", \"Costo\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n con datos sint√©ticos\n",
    "print(\"üéØ Entrenando modelo con Gradient Descent...\")\n",
    "np.random.seed(42)\n",
    "X_gd = np.random.randn(100, 3)  # 3 caracter√≠sticas\n",
    "true_theta = np.array([2, -1, 0.5, 3])  # Incluye intercepto\n",
    "y_gd = X_gd.dot(true_theta[1:]) + true_theta[0] + np.random.randn(100) * 0.5\n",
    "\n",
    "# Entrenar\n",
    "gd_model = GradientDescentRegression(learning_rate=0.1, \n",
    "                                     n_iterations=500, \n",
    "                                     verbose=True)\n",
    "gd_model.fit(X_gd, y_gd)\n",
    "\n",
    "print(f\"\\n‚úÖ Par√°metros verdaderos: {true_theta}\")\n",
    "print(f\"‚úÖ Par√°metros encontrados: {gd_model.theta.flatten()}\")\n",
    "\n",
    "# Visualizar convergencia\n",
    "gd_model.plot_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Experimento: Efecto del Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"4.2 EXPERIMENTO: EFECTO DEL LEARNING RATE\")\n",
    "\n",
    "def experiment_learning_rates():\n",
    "    \"\"\"Comparar diferentes learning rates\"\"\"\n",
    "    \n",
    "    learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, lr in enumerate(learning_rates):\n",
    "        # Entrenar modelo\n",
    "        model = GradientDescentRegression(learning_rate=lr, \n",
    "                                         n_iterations=200, \n",
    "                                         verbose=False)\n",
    "        model.fit(X_gd, y_gd)\n",
    "        \n",
    "        # Visualizar\n",
    "        ax = axes[idx]\n",
    "        ax.plot(model.cost_history, linewidth=2, \n",
    "               color=COLORS['accent'] if lr <= 0.1 else COLORS['warning'])\n",
    "        ax.set_xlabel('Iteraci√≥n')\n",
    "        ax.set_ylabel('Costo J(Œ∏)')\n",
    "        ax.set_title(f'Learning Rate = {lr}', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # A√±adir informaci√≥n\n",
    "        if lr <= 0.1:\n",
    "            final_cost = model.cost_history[-1]\n",
    "            ax.text(0.6, 0.8, f'Converge a {final_cost:.4f}',\n",
    "                   transform=ax.transAxes,\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "        else:\n",
    "            ax.text(0.6, 0.8, 'Diverge o oscila',\n",
    "                   transform=ax.transAxes,\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
    "    \n",
    "    plt.suptitle(\"Impacto del Learning Rate en la Convergencia\", \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üí° Observaciones:\")\n",
    "    print(\"  ‚Ä¢ Œ± muy peque√±o (0.001): Convergencia muy lenta\")\n",
    "    print(\"  ‚Ä¢ Œ± √≥ptimo (0.01-0.1): Convergencia r√°pida y estable\")\n",
    "    print(\"  ‚Ä¢ Œ± muy grande (>0.5): Divergencia o oscilaci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_learning_rates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Regularizaci√≥n\n",
    "\n",
    "### üìñ Teor√≠a: Control de Complejidad\n",
    "\n",
    "La regularizaci√≥n a√±ade una penalizaci√≥n al costo para evitar overfitting:\n",
    "\n",
    "- **Ridge (L2)**: $J = MSE + \\lambda \\sum \\theta_i^2$\n",
    "- **LASSO (L1)**: $J = MSE + \\lambda \\sum |\\theta_i|$\n",
    "- **Elastic Net**: Combina L1 y L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª Comparaci√≥n de T√©cnicas de Regularizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"6.1 COMPARACI√ìN DE REGULARIZACI√ìN\")\n",
    "\n",
    "def compare_regularization():\n",
    "    \"\"\"Comparar Ridge, LASSO y Elastic Net\"\"\"\n",
    "    \n",
    "    # Crear dataset con multicolinealidad\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 100, 20\n",
    "    X_reg = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # A√±adir correlaci√≥n entre features\n",
    "    X_reg[:, 1] = X_reg[:, 0] + np.random.randn(n_samples) * 0.1\n",
    "    X_reg[:, 2] = X_reg[:, 0] + np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    # Generar y con solo algunas features relevantes\n",
    "    true_coef = np.zeros(n_features)\n",
    "    true_coef[0:5] = [3, 2, -1.5, 0, -2]  # Solo 5 features relevantes\n",
    "    y_reg = X_reg @ true_coef + np.random.randn(n_samples) * 0.5\n",
    "    \n",
    "    # Split datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reg, y_reg, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Escalar\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Modelos\n",
    "    models = {\n",
    "        'Linear': LinearRegression(),\n",
    "        'Ridge': Ridge(alpha=1.0),\n",
    "        'LASSO': Lasso(alpha=0.1),\n",
    "        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    coefficients = {}\n",
    "    \n",
    "    # Entrenar modelos\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        results[name] = {\n",
    "            'train_score': model.score(X_train_scaled, y_train),\n",
    "            'test_score': model.score(X_test_scaled, y_test),\n",
    "            'mse': mean_squared_error(y_test, y_pred),\n",
    "            'n_zero_coef': np.sum(np.abs(model.coef_) < 0.01)\n",
    "        }\n",
    "        coefficients[name] = model.coef_\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Comparaci√≥n de scores\n",
    "    models_names = list(results.keys())\n",
    "    train_scores = [results[m]['train_score'] for m in models_names]\n",
    "    test_scores = [results[m]['test_score'] for m in models_names]\n",
    "    \n",
    "    x = np.arange(len(models_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x - width/2, train_scores, width, \n",
    "                   label='Train', color=COLORS['primary'])\n",
    "    axes[0, 0].bar(x + width/2, test_scores, width, \n",
    "                   label='Test', color=COLORS['accent'])\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(models_names)\n",
    "    axes[0, 0].set_ylim([0, 1])\n",
    "    plot_style(axes[0, 0], \"Comparaci√≥n de R¬≤ Score\", \"\", \"R¬≤\")\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Coeficientes\n",
    "    for name, coef in coefficients.items():\n",
    "        axes[0, 1].plot(coef, 'o-', label=name, alpha=0.7, markersize=4)\n",
    "    axes[0, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plot_style(axes[0, 1], \"Valores de Coeficientes\", \"Feature\", \"Coeficiente\")\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. N√∫mero de coeficientes cero\n",
    "    zero_counts = [results[m]['n_zero_coef'] for m in models_names]\n",
    "    axes[1, 0].bar(models_names, zero_counts, color=COLORS['warning'])\n",
    "    plot_style(axes[1, 0], \"Coeficientes ‚âà 0 (Sparsity)\", \n",
    "              \"Modelo\", \"N√∫mero de Coef ‚âà 0\")\n",
    "    \n",
    "    # A√±adir valores\n",
    "    for i, (name, count) in enumerate(zip(models_names, zero_counts)):\n",
    "        axes[1, 0].text(i, count + 0.5, str(count), ha='center')\n",
    "    \n",
    "    # 4. MSE Comparison\n",
    "    mse_values = [results[m]['mse'] for m in models_names]\n",
    "    axes[1, 1].bar(models_names, mse_values, color=COLORS['secondary'])\n",
    "    plot_style(axes[1, 1], \"Mean Squared Error\", \"Modelo\", \"MSE\")\n",
    "    \n",
    "    plt.suptitle(\"Comparaci√≥n de T√©cnicas de Regularizaci√≥n\", \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla de resultados\n",
    "    print(\"\\nüìä Tabla de Resultados:\")\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    display(results_df)\n",
    "    \n",
    "    print(\"\\nüí° Observaciones:\")\n",
    "    print(\"  ‚Ä¢ Linear: Puede overfittear con muchas features\")\n",
    "    print(\"  ‚Ä¢ Ridge: Reduce magnitud pero mantiene todas las features\")\n",
    "    print(\"  ‚Ä¢ LASSO: Selecciona features (pone algunas en 0)\")\n",
    "    print(\"  ‚Ä¢ ElasticNet: Balance entre Ridge y LASSO\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Regresi√≥n Log√≠stica\n",
    "\n",
    "### üìñ Teor√≠a: De Regresi√≥n a Clasificaci√≥n\n",
    "\n",
    "La regresi√≥n log√≠stica usa la funci√≥n sigmoide para convertir valores reales en probabilidades:\n",
    "\n",
    "$$P(y=1|x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª Implementaci√≥n desde Cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"7.1 REGRESI√ìN LOG√çSTICA - IMPLEMENTACI√ìN MANUAL\")\n",
    "\n",
    "class LogisticRegressionManual:\n",
    "    \"\"\"\n",
    "    Implementaci√≥n de Regresi√≥n Log√≠stica usando Gradient Descent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Funci√≥n sigmoide\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip para estabilidad\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"Agregar columna de unos\"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.c_[intercept, X]\n",
    "    \n",
    "    def _cost_function(self, X, y, theta):\n",
    "        \"\"\"Funci√≥n de costo (cross-entropy)\"\"\"\n",
    "        m = len(y)\n",
    "        z = X.dot(theta)\n",
    "        predictions = self._sigmoid(z)\n",
    "        \n",
    "        # Evitar log(0)\n",
    "        epsilon = 1e-7\n",
    "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "        \n",
    "        cost = -(1/m) * np.sum(y * np.log(predictions) + \n",
    "                               (1 - y) * np.log(1 - predictions))\n",
    "        return cost\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrenar el modelo\"\"\"\n",
    "        X = self._add_intercept(X)\n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Inicializar par√°metros\n",
    "        self.theta = np.zeros(n)\n",
    "        \n",
    "        # Gradient Descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Predicciones\n",
    "            z = X.dot(self.theta)\n",
    "            predictions = self._sigmoid(z)\n",
    "            \n",
    "            # Gradientes\n",
    "            gradients = (1/m) * X.T.dot(predictions - y)\n",
    "            \n",
    "            # Actualizar par√°metros\n",
    "            self.theta -= self.learning_rate * gradients\n",
    "            \n",
    "            # Guardar costo\n",
    "            cost = self._cost_function(X, y, self.theta)\n",
    "            self.cost_history.append(cost)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predecir probabilidades\"\"\"\n",
    "        X = self._add_intercept(X)\n",
    "        return self._sigmoid(X.dot(self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predecir clases\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "    \n",
    "    def plot_decision_boundary(self, X, y):\n",
    "        \"\"\"Visualizar frontera de decisi√≥n (2D)\"\"\"\n",
    "        if X.shape[1] != 2:\n",
    "            print(\"‚ö†Ô∏è Solo se puede visualizar para 2 features\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Plot puntos\n",
    "        colors = ['blue' if yi == 0 else 'red' for yi in y]\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.6, \n",
    "                  edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        # Crear mesh para frontera\n",
    "        h = 0.02\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                            np.arange(y_min, y_max, h))\n",
    "        \n",
    "        # Predicciones en mesh\n",
    "        Z = self.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Contorno\n",
    "        ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu', levels=[0, 0.5, 1])\n",
    "        ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "        \n",
    "        plot_style(ax, \"Frontera de Decisi√≥n - Regresi√≥n Log√≠stica\", \n",
    "                  \"Feature 1\", \"Feature 2\")\n",
    "        \n",
    "        # Leyenda\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='blue', alpha=0.6, label='Clase 0'),\n",
    "            Patch(facecolor='red', alpha=0.6, label='Clase 1')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n con datos sint√©ticos\n",
    "print(\"üéØ Generando dataset de clasificaci√≥n binaria...\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generar dos clases separables\n",
    "n_samples = 200\n",
    "X_class1 = np.random.randn(n_samples//2, 2) + np.array([2, 2])\n",
    "X_class2 = np.random.randn(n_samples//2, 2) + np.array([-2, -2])\n",
    "X_log = np.vstack([X_class1, X_class2])\n",
    "y_log = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])\n",
    "\n",
    "# Entrenar modelo\n",
    "log_model = LogisticRegressionManual(learning_rate=0.1, n_iterations=500)\n",
    "log_model.fit(X_log, y_log)\n",
    "\n",
    "print(f\"‚úÖ Modelo entrenado\")\n",
    "print(f\"   Par√°metros: Œ∏‚ÇÄ={log_model.theta[0]:.3f}, Œ∏‚ÇÅ={log_model.theta[1]:.3f}, Œ∏‚ÇÇ={log_model.theta[2]:.3f}\")\n",
    "\n",
    "# Evaluar\n",
    "y_pred = log_model.predict(X_log)\n",
    "accuracy = np.mean(y_pred == y_log)\n",
    "print(f\"   Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Visualizar frontera\n",
    "log_model.plot_decision_boundary(X_log, y_log)\n",
    "\n",
    "# Convergencia\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(log_model.cost_history, color=COLORS['primary'], linewidth=2)\n",
    "plot_style(ax, \"Convergencia de Regresi√≥n Log√≠stica\", \"Iteraci√≥n\", \"Costo (Cross-Entropy)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Funci√≥n Sigmoide y Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"7.2 EXPLORANDO LA FUNCI√ìN SIGMOIDE\")\n",
    "\n",
    "def explore_sigmoid():\n",
    "    \"\"\"Visualizaci√≥n interactiva de la funci√≥n sigmoide\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Funci√≥n sigmoide b√°sica\n",
    "    z = np.linspace(-10, 10, 1000)\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    axes[0, 0].plot(z, sigmoid, color=COLORS['primary'], linewidth=3)\n",
    "    axes[0, 0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].fill_between(z[z <= 0], 0, sigmoid[z <= 0], \n",
    "                           alpha=0.3, color=COLORS['primary'], label='P < 0.5')\n",
    "    axes[0, 0].fill_between(z[z >= 0], sigmoid[z >= 0], 1, \n",
    "                           alpha=0.3, color=COLORS['accent'], label='P > 0.5')\n",
    "    plot_style(axes[0, 0], \"Funci√≥n Sigmoide\", \"z = Œ∏·µÄx\", \"P(y=1)\")\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Derivada de la sigmoide\n",
    "    sigmoid_derivative = sigmoid * (1 - sigmoid)\n",
    "    axes[0, 1].plot(z, sigmoid_derivative, color=COLORS['secondary'], linewidth=3)\n",
    "    axes[0, 1].fill_between(z, 0, sigmoid_derivative, alpha=0.3, color=COLORS['secondary'])\n",
    "    plot_style(axes[0, 1], \"Derivada de Sigmoide\", \"z\", \"œÉ'(z) = œÉ(z)(1-œÉ(z))\")\n",
    "    axes[0, 1].text(0, 0.22, 'M√°ximo en z=0\\nœÉ\\'(0) = 0.25', ha='center',\n",
    "                   bbox=dict(boxstyle='round', facecolor='yellow'))\n",
    "    \n",
    "    # 3. Log-Odds (Logit)\n",
    "    p = np.linspace(0.01, 0.99, 100)\n",
    "    log_odds = np.log(p / (1 - p))\n",
    "    \n",
    "    axes[1, 0].plot(p, log_odds, color=COLORS['warning'], linewidth=3)\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[1, 0].axvline(x=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    plot_style(axes[1, 0], \"Log-Odds (Logit)\", \"Probabilidad\", \"log(p/(1-p))\")\n",
    "    axes[1, 0].set_ylim([-5, 5])\n",
    "    \n",
    "    # 4. Comparaci√≥n de diferentes pendientes\n",
    "    z_comp = np.linspace(-5, 5, 100)\n",
    "    for beta in [0.5, 1, 2, 4]:\n",
    "        sigmoid_beta = 1 / (1 + np.exp(-beta * z_comp))\n",
    "        axes[1, 1].plot(z_comp, sigmoid_beta, linewidth=2, label=f'Œ≤={beta}')\n",
    "    \n",
    "    axes[1, 1].axhline(y=0.5, color='black', linestyle='--', alpha=0.3)\n",
    "    axes[1, 1].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "    plot_style(axes[1, 1], \"Efecto de la Pendiente\", \"z\", \"P(y=1)\")\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.suptitle(\"Explorando la Funci√≥n Sigmoide\", fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üí° Observaciones clave:\")\n",
    "    print(\"  ‚Ä¢ La sigmoide mapea (-‚àû, +‚àû) ‚Üí (0, 1)\")\n",
    "    print(\"  ‚Ä¢ Derivada m√°xima en z=0 (punto de mayor incertidumbre)\")\n",
    "    print(\"  ‚Ä¢ Mayor pendiente Œ≤ ‚Üí transici√≥n m√°s abrupta\")\n",
    "    print(\"  ‚Ä¢ Log-odds son lineales en las caracter√≠sticas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. M√©tricas de Clasificaci√≥n\n",
    "\n",
    "### üìñ Teor√≠a: Evaluando Clasificadores\n",
    "\n",
    "M√©tricas clave:\n",
    "- **Accuracy**: % de predicciones correctas\n",
    "- **Precision**: De los que predije positivo, ¬øcu√°ntos lo eran?\n",
    "- **Recall**: De los positivos reales, ¬øcu√°ntos detect√©?\n",
    "- **F1-Score**: Media arm√≥nica de precision y recall\n",
    "- **AUC-ROC**: √Årea bajo la curva ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª An√°lisis Completo de M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"8.1 M√âTRICAS DE CLASIFICACI√ìN - DATASET REAL\")\n",
    "\n",
    "# Cargar dataset de c√°ncer de mama\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = cancer.data\n",
    "y_cancer = cancer.target\n",
    "\n",
    "print(\"üìä Dataset de C√°ncer de Mama:\")\n",
    "print(f\"  ‚Ä¢ Muestras: {X_cancer.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_cancer.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Clases: {cancer.target_names}\")\n",
    "print(f\"  ‚Ä¢ Balance: {np.bincount(y_cancer)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split y escalar\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar modelo\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "y_proba = log_reg.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_metrics(y_true, y_pred, y_proba):\n",
    "    \"\"\"Visualizaci√≥n completa de m√©tricas de clasificaci√≥n\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Matriz de Confusi√≥n\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                cbar=False, ax=axes[0, 0])\n",
    "    axes[0, 0].set_xlabel('Predicho')\n",
    "    axes[0, 0].set_ylabel('Real')\n",
    "    axes[0, 0].set_title('Matriz de Confusi√≥n', fontweight='bold')\n",
    "    \n",
    "    # A√±adir m√©tricas\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    axes[0, 0].text(2.5, 1, f'Sens: {sensitivity:.2%}\\nSpec: {specificity:.2%}',\n",
    "                   bbox=dict(boxstyle='round', facecolor='yellow'))\n",
    "    \n",
    "    # 2. Curva ROC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    axes[0, 1].plot(fpr, tpr, color=COLORS['primary'], linewidth=2,\n",
    "                   label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    axes[0, 1].fill_between(fpr, tpr, alpha=0.3, color=COLORS['primary'])\n",
    "    axes[0, 1].set_xlabel('False Positive Rate')\n",
    "    axes[0, 1].set_ylabel('True Positive Rate')\n",
    "    axes[0, 1].set_title('Curva ROC', fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Precision-Recall Curve\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    avg_precision = average_precision_score(y_true, y_proba)\n",
    "    \n",
    "    axes[0, 2].plot(recall, precision, color=COLORS['accent'], linewidth=2,\n",
    "                   label=f'AP = {avg_precision:.3f}')\n",
    "    axes[0, 2].fill_between(recall, precision, alpha=0.3, color=COLORS['accent'])\n",
    "    axes[0, 2].set_xlabel('Recall')\n",
    "    axes[0, 2].set_ylabel('Precision')\n",
    "    axes[0, 2].set_title('Curva Precision-Recall', fontweight='bold')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Distribuci√≥n de Probabilidades\n",
    "    axes[1, 0].hist(y_proba[y_true == 0], bins=30, alpha=0.7, \n",
    "                   color=COLORS['primary'], label='Clase 0', density=True)\n",
    "    axes[1, 0].hist(y_proba[y_true == 1], bins=30, alpha=0.7, \n",
    "                   color=COLORS['warning'], label='Clase 1', density=True)\n",
    "    axes[1, 0].axvline(x=0.5, color='black', linestyle='--', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Probabilidad Predicha')\n",
    "    axes[1, 0].set_ylabel('Densidad')\n",
    "    axes[1, 0].set_title('Distribuci√≥n de Probabilidades', fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. M√©tricas por Umbral\n",
    "    thresholds_plot = np.linspace(0.1, 0.9, 50)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for thresh in thresholds_plot:\n",
    "        y_pred_thresh = (y_proba >= thresh).astype(int)\n",
    "        if len(np.unique(y_pred_thresh)) > 1:  # Evitar warnings\n",
    "            prec = precision_score(y_true, y_pred_thresh, zero_division=0)\n",
    "            rec = recall_score(y_true, y_pred_thresh)\n",
    "            f1 = f1_score(y_true, y_pred_thresh)\n",
    "        else:\n",
    "            prec = rec = f1 = 0\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    axes[1, 1].plot(thresholds_plot, precisions, label='Precision', linewidth=2)\n",
    "    axes[1, 1].plot(thresholds_plot, recalls, label='Recall', linewidth=2)\n",
    "    axes[1, 1].plot(thresholds_plot, f1s, label='F1-Score', linewidth=2)\n",
    "    axes[1, 1].axvline(x=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].set_xlabel('Umbral')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('M√©tricas vs Umbral', fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Classification Report\n",
    "    from sklearn.metrics import classification_report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    # Crear tabla\n",
    "    metrics_data = []\n",
    "    for label in ['0', '1']:\n",
    "        metrics_data.append([\n",
    "            f\"Clase {label}\",\n",
    "            f\"{report[label]['precision']:.3f}\",\n",
    "            f\"{report[label]['recall']:.3f}\",\n",
    "            f\"{report[label]['f1-score']:.3f}\",\n",
    "            f\"{report[label]['support']:.0f}\"\n",
    "        ])\n",
    "    metrics_data.append([\n",
    "        \"Weighted Avg\",\n",
    "        f\"{report['weighted avg']['precision']:.3f}\",\n",
    "        f\"{report['weighted avg']['recall']:.3f}\",\n",
    "        f\"{report['weighted avg']['f1-score']:.3f}\",\n",
    "        f\"{report['weighted avg']['support']:.0f}\"\n",
    "    ])\n",
    "    \n",
    "    # Mostrar tabla\n",
    "    axes[1, 2].axis('tight')\n",
    "    axes[1, 2].axis('off')\n",
    "    table = axes[1, 2].table(cellText=metrics_data,\n",
    "                            colLabels=['Clase', 'Precision', 'Recall', 'F1', 'Support'],\n",
    "                            cellLoc='center',\n",
    "                            loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    axes[1, 2].set_title('Classification Report', fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.suptitle(\"An√°lisis Completo de M√©tricas de Clasificaci√≥n\", \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Imprimir m√©tricas adicionales\n",
    "    print(\"\\nüìä M√©tricas Globales:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"  ‚Ä¢ AUC-ROC: {roc_auc:.3f}\")\n",
    "    print(f\"  ‚Ä¢ Average Precision: {avg_precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(y_test, y_pred, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Casos Pr√°cticos Reales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Caso 1: Predicci√≥n de Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"9.1 CASO PR√ÅCTICO: PREDICCI√ìN DE DIABETES\")\n",
    "\n",
    "# Cargar dataset de diabetes\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "\n",
    "print(\"üìä Dataset de Diabetes:\")\n",
    "print(f\"  ‚Ä¢ Muestras: {X_diabetes.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_diabetes.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Variable objetivo: Progresi√≥n de diabetes (continua)\")\n",
    "\n",
    "# Informaci√≥n de features\n",
    "feature_names = ['age', 'sex', 'bmi', 'bp', 'tc', 'ldl', 'hdl', 'tch', 'ltg', 'glu']\n",
    "print(\"\\nüìã Features (todas normalizadas):\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i+1}. {name}\")\n",
    "\n",
    "# Pipeline completo\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Crear pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "\n",
    "# Grid search para hyperpar√°metros\n",
    "param_grid = {\n",
    "    'model__alpha': np.logspace(-3, 3, 20)\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, \n",
    "                          scoring='neg_mean_squared_error',\n",
    "                          n_jobs=-1)\n",
    "\n",
    "# Split datos\n",
    "X_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n",
    "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "print(\"\\nüîç Realizando Grid Search con Cross-Validation...\")\n",
    "grid_search.fit(X_train_diab, y_train_diab)\n",
    "\n",
    "print(f\"\\n‚úÖ Mejores hiperpar√°metros: {grid_search.best_params_}\")\n",
    "print(f\"‚úÖ Mejor score CV: {-grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Evaluar en test\n",
    "y_pred_diab = grid_search.predict(X_test_diab)\n",
    "test_mse = mean_squared_error(y_test_diab, y_pred_diab)\n",
    "test_r2 = r2_score(y_test_diab, y_pred_diab)\n",
    "\n",
    "print(f\"\\nüìä Resultados en Test:\")\n",
    "print(f\"  ‚Ä¢ MSE: {test_mse:.2f}\")\n",
    "print(f\"  ‚Ä¢ RMSE: {np.sqrt(test_mse):.2f}\")\n",
    "print(f\"  ‚Ä¢ R¬≤: {test_r2:.3f}\")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# CV scores\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "axes[0].plot(cv_results['param_model__alpha'], \n",
    "            -cv_results['mean_test_score'], \n",
    "            'o-', color=COLORS['primary'])\n",
    "axes[0].fill_between(cv_results['param_model__alpha'],\n",
    "                     -cv_results['mean_test_score'] - cv_results['std_test_score'],\n",
    "                     -cv_results['mean_test_score'] + cv_results['std_test_score'],\n",
    "                     alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Œ± (Regularizaci√≥n)')\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].axvline(x=grid_search.best_params_['model__alpha'], \n",
    "               color='red', linestyle='--')\n",
    "plot_style(axes[0], \"Cross-Validation Scores\", \"Œ±\", \"MSE\")\n",
    "\n",
    "# Predicciones\n",
    "axes[1].scatter(y_test_diab, y_pred_diab, alpha=0.6, \n",
    "               color=COLORS['accent'], edgecolors='black', linewidth=0.5)\n",
    "axes[1].plot([y_test_diab.min(), y_test_diab.max()],\n",
    "            [y_test_diab.min(), y_test_diab.max()],\n",
    "            'r--', linewidth=2)\n",
    "plot_style(axes[1], \"Predicciones en Test\", \"Real\", \"Predicci√≥n\")\n",
    "\n",
    "# Feature importance\n",
    "best_model = grid_search.best_estimator_['model']\n",
    "importance = np.abs(best_model.coef_)\n",
    "sorted_idx = np.argsort(importance)[::-1]\n",
    "\n",
    "axes[2].barh(range(len(feature_names)), importance[sorted_idx], \n",
    "            color=COLORS['secondary'])\n",
    "axes[2].set_yticks(range(len(feature_names)))\n",
    "axes[2].set_yticklabels([feature_names[i] for i in sorted_idx])\n",
    "plot_style(axes[2], \"Importancia de Features\", \"|Coeficiente|\", \"\")\n",
    "\n",
    "plt.suptitle(\"Predicci√≥n de Progresi√≥n de Diabetes\", \n",
    "            fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üè† Caso 2: Sistema Completo de Predicci√≥n de Precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"9.2 SISTEMA COMPLETO: PREDICCI√ìN DE PRECIOS INMOBILIARIOS\")\n",
    "\n",
    "class RealEstatePricePredictor:\n",
    "    \"\"\"\n",
    "    Sistema completo de predicci√≥n de precios con:\n",
    "    - Preprocesamiento autom√°tico\n",
    "    - Selecci√≥n de features\n",
    "    - Multiple models\n",
    "    - Interpretabilidad\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.feature_names = None\n",
    "        self.results = {}\n",
    "        \n",
    "    def preprocess(self, X, y=None, fit=True):\n",
    "        \"\"\"Preprocesamiento de datos\"\"\"\n",
    "        if fit:\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        return X_scaled\n",
    "    \n",
    "    def train_multiple_models(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Entrenar y comparar m√∫ltiples modelos\"\"\"\n",
    "        \n",
    "        models_to_try = {\n",
    "            'Linear': LinearRegression(),\n",
    "            'Ridge': RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "            'Lasso': LassoCV(alphas=np.logspace(-3, 1, 10), max_iter=1000),\n",
    "            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "            'Polynomial': Pipeline([\n",
    "                ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "                ('linear', Ridge(alpha=1))\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        for name, model in models_to_try.items():\n",
    "            # Entrenar\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predicciones\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            \n",
    "            # M√©tricas\n",
    "            self.results[name] = {\n",
    "                'model': model,\n",
    "                'train_r2': r2_score(y_train, y_train_pred),\n",
    "                'val_r2': r2_score(y_val, y_val_pred),\n",
    "                'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "                'val_rmse': np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "            }\n",
    "            \n",
    "        # Seleccionar mejor modelo\n",
    "        best_name = max(self.results.keys(), \n",
    "                       key=lambda k: self.results[k]['val_r2'])\n",
    "        self.best_model = self.results[best_name]['model']\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Visualizar comparaci√≥n de modelos\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Preparar datos\n",
    "        model_names = list(self.results.keys())\n",
    "        train_r2 = [self.results[m]['train_r2'] for m in model_names]\n",
    "        val_r2 = [self.results[m]['val_r2'] for m in model_names]\n",
    "        train_rmse = [self.results[m]['train_rmse'] for m in model_names]\n",
    "        val_rmse = [self.results[m]['val_rmse'] for m in model_names]\n",
    "        \n",
    "        # R¬≤ Comparison\n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0, 0].bar(x - width/2, train_r2, width, label='Train', \n",
    "                      color=COLORS['primary'])\n",
    "        axes[0, 0].bar(x + width/2, val_r2, width, label='Validation', \n",
    "                      color=COLORS['accent'])\n",
    "        axes[0, 0].set_xticks(x)\n",
    "        axes[0, 0].set_xticklabels(model_names, rotation=45)\n",
    "        axes[0, 0].set_ylim([0, 1])\n",
    "        plot_style(axes[0, 0], \"R¬≤ Score Comparison\", \"\", \"R¬≤\")\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # RMSE Comparison\n",
    "        axes[0, 1].bar(x - width/2, train_rmse, width, label='Train', \n",
    "                      color=COLORS['primary'])\n",
    "        axes[0, 1].bar(x + width/2, val_rmse, width, label='Validation', \n",
    "                      color=COLORS['accent'])\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels(model_names, rotation=45)\n",
    "        plot_style(axes[0, 1], \"RMSE Comparison\", \"\", \"RMSE\")\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Overfitting analysis\n",
    "        overfitting = np.array(train_r2) - np.array(val_r2)\n",
    "        colors = [COLORS['success'] if o < 0.1 else COLORS['warning'] \n",
    "                 if o < 0.2 else COLORS['warning'] for o in overfitting]\n",
    "        axes[1, 0].bar(model_names, overfitting, color=colors)\n",
    "        axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        axes[1, 0].set_xticklabels(model_names, rotation=45)\n",
    "        plot_style(axes[1, 0], \"Overfitting Analysis\", \"\", \"Train R¬≤ - Val R¬≤\")\n",
    "        \n",
    "        # Summary table\n",
    "        axes[1, 1].axis('tight')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        table_data = []\n",
    "        for name in model_names:\n",
    "            table_data.append([\n",
    "                name,\n",
    "                f\"{self.results[name]['train_r2']:.3f}\",\n",
    "                f\"{self.results[name]['val_r2']:.3f}\",\n",
    "                f\"{self.results[name]['val_rmse']:.1f}\"\n",
    "            ])\n",
    "        \n",
    "        table = axes[1, 1].table(cellText=table_data,\n",
    "                                colLabels=['Modelo', 'Train R¬≤', 'Val R¬≤', 'Val RMSE'],\n",
    "                                cellLoc='center',\n",
    "                                loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(11)\n",
    "        table.scale(1.2, 2)\n",
    "        \n",
    "        # Highlight best model\n",
    "        best_idx = val_r2.index(max(val_r2))\n",
    "        for j in range(4):\n",
    "            table[(best_idx + 1, j)].set_facecolor('#90EE90')\n",
    "        \n",
    "        plt.suptitle(\"Comparaci√≥n de Modelos de Regresi√≥n\", \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Aplicar sistema\n",
    "print(\"üèóÔ∏è Creando sistema de predicci√≥n de precios...\")\n",
    "\n",
    "# Usar California Housing\n",
    "X_train_cal, X_test_cal, y_train_cal, y_test_cal = train_test_split(\n",
    "    X_calif, y_calif, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Crear validaci√≥n set\n",
    "X_train_final, X_val_cal, y_train_final, y_val_cal = train_test_split(\n",
    "    X_train_cal, y_train_cal, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Inicializar sistema\n",
    "predictor = RealEstatePricePredictor()\n",
    "predictor.feature_names = X_calif.columns\n",
    "\n",
    "# Preprocesar\n",
    "X_train_proc = predictor.preprocess(X_train_final)\n",
    "X_val_proc = predictor.preprocess(X_val_cal, fit=False)\n",
    "\n",
    "# Entrenar modelos\n",
    "print(\"üéØ Entrenando m√∫ltiples modelos...\")\n",
    "results = predictor.train_multiple_models(X_train_proc, y_train_final,\n",
    "                                         X_val_proc, y_val_cal)\n",
    "\n",
    "# Visualizar\n",
    "predictor.plot_results()\n",
    "\n",
    "# Mejor modelo\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['val_r2'])\n",
    "print(f\"\\nüèÜ Mejor modelo: {best_model_name}\")\n",
    "print(f\"   Val R¬≤: {results[best_model_name]['val_r2']:.3f}\")\n",
    "print(f\"   Val RMSE: {results[best_model_name]['val_rmse']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Proyecto Final Integrador\n",
    "\n",
    "### üéØ Proyecto: Sistema de Scoring de Cr√©dito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"10. PROYECTO FINAL: SISTEMA DE SCORING DE CR√âDITO\")\n",
    "\n",
    "print(\"\"\"\n",
    "üìã DESCRIPCI√ìN DEL PROYECTO:\n",
    "Construir un sistema completo de scoring crediticio que:\n",
    "1. Prediga la probabilidad de default de un cliente\n",
    "2. Asigne un score de cr√©dito (300-850)\n",
    "3. Determine el l√≠mite de cr√©dito apropiado\n",
    "4. Proporcione explicaciones interpretables\n",
    "\"\"\")\n",
    "\n",
    "# Generar dataset sint√©tico de cr√©dito\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "# Features\n",
    "credit_data = pd.DataFrame({\n",
    "    'age': np.random.normal(40, 15, n_samples).clip(18, 80),\n",
    "    'income': np.random.lognormal(10.5, 0.8, n_samples),\n",
    "    'employment_years': np.random.exponential(5, n_samples).clip(0, 40),\n",
    "    'debt_to_income': np.random.beta(2, 5, n_samples),\n",
    "    'credit_inquiries': np.random.poisson(2, n_samples),\n",
    "    'credit_accounts': np.random.poisson(5, n_samples) + 1,\n",
    "    'missed_payments': np.random.poisson(0.5, n_samples),\n",
    "    'credit_utilization': np.random.beta(2, 3, n_samples),\n",
    "    'bankruptcy': np.random.binomial(1, 0.05, n_samples),\n",
    "    'home_owner': np.random.binomial(1, 0.6, n_samples)\n",
    "})\n",
    "\n",
    "# Target (probabilidad base de default)\n",
    "default_prob_base = (\n",
    "    0.5 * credit_data['debt_to_income'] +\n",
    "    0.3 * credit_data['credit_utilization'] +\n",
    "    0.1 * (credit_data['missed_payments'] / 10) +\n",
    "    0.2 * credit_data['bankruptcy'] -\n",
    "    0.1 * (credit_data['income'] / credit_data['income'].max()) -\n",
    "    0.1 * credit_data['home_owner'] +\n",
    "    np.random.normal(0, 0.1, n_samples)\n",
    ")\n",
    "\n",
    "# Convertir a probabilidad y luego a binario\n",
    "default_prob = 1 / (1 + np.exp(-3 * (default_prob_base - 0.3)))\n",
    "credit_data['default'] = (default_prob > np.random.random(n_samples)).astype(int)\n",
    "\n",
    "print(\"üìä Dataset de Cr√©dito Generado:\")\n",
    "print(f\"  ‚Ä¢ Muestras: {len(credit_data):,}\")\n",
    "print(f\"  ‚Ä¢ Features: {len(credit_data.columns) - 1}\")\n",
    "print(f\"  ‚Ä¢ Tasa de default: {credit_data['default'].mean():.1%}\")\n",
    "\n",
    "# Exploraci√≥n inicial\n",
    "print(\"\\nüìà Estad√≠sticas Descriptivas:\")\n",
    "display(credit_data.describe())\n",
    "\n",
    "# An√°lisis de default por caracter√≠stica\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "features_to_analyze = ['age', 'income', 'debt_to_income', \n",
    "                       'credit_utilization', 'missed_payments', 'credit_inquiries']\n",
    "\n",
    "for idx, feature in enumerate(features_to_analyze):\n",
    "    # Crear bins para an√°lisis\n",
    "    if feature in ['age', 'income']:\n",
    "        bins = pd.qcut(credit_data[feature], q=5, duplicates='drop')\n",
    "    else:\n",
    "        bins = pd.cut(credit_data[feature], bins=5)\n",
    "    \n",
    "    # Calcular tasa de default por bin\n",
    "    default_rate = credit_data.groupby(bins)['default'].mean()\n",
    "    \n",
    "    axes[idx].bar(range(len(default_rate)), default_rate.values, \n",
    "                 color=COLORS['warning'], alpha=0.7)\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Tasa de Default')\n",
    "    axes[idx].set_title(f'Default vs {feature}', fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotar labels si es necesario\n",
    "    if idx < 2:\n",
    "        axes[idx].set_xticklabels([f'{int(b.left)}-{int(b.right)}' \n",
    "                                   for b in default_rate.index], rotation=45)\n",
    "\n",
    "plt.suptitle(\"An√°lisis Exploratorio de Default\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "class CreditScoringSystem:\n",
    "    \"\"\"\n",
    "    Sistema completo de scoring crediticio\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_importance = None\n",
    "        \n",
    "    def prepare_features(self, data):\n",
    "        \"\"\"Ingenier√≠a de caracter√≠sticas\"\"\"\n",
    "        features = data.copy()\n",
    "        \n",
    "        # Crear nuevas features\n",
    "        features['income_per_year'] = features['income'] / (features['age'] - 18 + 1)\n",
    "        features['payment_history_score'] = (\n",
    "            1 - features['missed_payments'] / (features['missed_payments'].max() + 1)\n",
    "        )\n",
    "        features['credit_age'] = features['employment_years'] * features['credit_accounts']\n",
    "        features['financial_stress'] = (\n",
    "            features['debt_to_income'] * features['credit_utilization']\n",
    "        )\n",
    "        \n",
    "        # Eliminar target si existe\n",
    "        if 'default' in features.columns:\n",
    "            features = features.drop('default', axis=1)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Entrenar modelo de scoring\"\"\"\n",
    "        \n",
    "        # Preparar features\n",
    "        X_prep = self.prepare_features(X_train)\n",
    "        \n",
    "        # Escalar\n",
    "        X_scaled = self.scaler.fit_transform(X_prep)\n",
    "        \n",
    "        # Entrenar modelo con regularizaci√≥n\n",
    "        self.model = LogisticRegression(\n",
    "            class_weight='balanced',\n",
    "            penalty='l2',\n",
    "            C=0.1,\n",
    "            max_iter=1000\n",
    "        )\n",
    "        self.model.fit(X_scaled, y_train)\n",
    "        \n",
    "        # Guardar importancia\n",
    "        self.feature_importance = pd.DataFrame({\n",
    "            'feature': X_prep.columns,\n",
    "            'importance': np.abs(self.model.coef_[0])\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_default_probability(self, X):\n",
    "        \"\"\"Predecir probabilidad de default\"\"\"\n",
    "        X_prep = self.prepare_features(X)\n",
    "        X_scaled = self.scaler.transform(X_prep)\n",
    "        return self.model.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    def calculate_credit_score(self, default_prob):\n",
    "        \"\"\"Convertir probabilidad en score (300-850)\"\"\"\n",
    "        # Mapeo inverso: menor probabilidad = mayor score\n",
    "        # Usando funci√≥n log√≠stica inversa\n",
    "        score = 300 + 550 * (1 - default_prob) ** 2\n",
    "        return np.clip(score, 300, 850).astype(int)\n",
    "    \n",
    "    def determine_credit_limit(self, income, score):\n",
    "        \"\"\"Determinar l√≠mite de cr√©dito basado en income y score\"\"\"\n",
    "        # Factor basado en score\n",
    "        score_factor = (score - 300) / 550\n",
    "        \n",
    "        # L√≠mite base: 20-50% del ingreso anual\n",
    "        base_limit = income * (0.2 + 0.3 * score_factor)\n",
    "        \n",
    "        # Ajustar por categor√≠as de score\n",
    "        if score >= 750:\n",
    "            multiplier = 1.5\n",
    "        elif score >= 700:\n",
    "            multiplier = 1.2\n",
    "        elif score >= 650:\n",
    "            multiplier = 1.0\n",
    "        elif score >= 600:\n",
    "            multiplier = 0.7\n",
    "        else:\n",
    "            multiplier = 0.3\n",
    "        \n",
    "        return int(base_limit * multiplier)\n",
    "    \n",
    "    def generate_report(self, customer_data):\n",
    "        \"\"\"Generar reporte completo de cr√©dito\"\"\"\n",
    "        \n",
    "        # Predicci√≥n\n",
    "        default_prob = self.predict_default_probability(customer_data)[0]\n",
    "        credit_score = self.calculate_credit_score(default_prob)\n",
    "        credit_limit = self.determine_credit_limit(\n",
    "            customer_data['income'].values[0], \n",
    "            credit_score\n",
    "        )\n",
    "        \n",
    "        # Categor√≠a de riesgo\n",
    "        if credit_score >= 750:\n",
    "            risk_category = \"Excelente\"\n",
    "            risk_color = \"green\"\n",
    "        elif credit_score >= 700:\n",
    "            risk_category = \"Bueno\"\n",
    "            risk_color = \"lightgreen\"\n",
    "        elif credit_score >= 650:\n",
    "            risk_category = \"Regular\"\n",
    "            risk_color = \"yellow\"\n",
    "        elif credit_score >= 600:\n",
    "            risk_category = \"Malo\"\n",
    "            risk_color = \"orange\"\n",
    "        else:\n",
    "            risk_category = \"Muy Malo\"\n",
    "            risk_color = \"red\"\n",
    "        \n",
    "        # Factores principales\n",
    "        X_prep = self.prepare_features(customer_data)\n",
    "        X_scaled = self.scaler.transform(X_prep)\n",
    "        contributions = X_scaled[0] * self.model.coef_[0]\n",
    "        \n",
    "        factors = pd.DataFrame({\n",
    "            'Factor': X_prep.columns,\n",
    "            'Valor': X_prep.values[0],\n",
    "            'Impacto': contributions\n",
    "        }).sort_values('Impacto', key=abs, ascending=False).head(5)\n",
    "        \n",
    "        return {\n",
    "            'credit_score': credit_score,\n",
    "            'default_probability': default_prob,\n",
    "            'risk_category': risk_category,\n",
    "            'risk_color': risk_color,\n",
    "            'credit_limit': credit_limit,\n",
    "            'main_factors': factors,\n",
    "            'recommendation': self._generate_recommendation(credit_score, factors)\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendation(self, score, factors):\n",
    "        \"\"\"Generar recomendaciones personalizadas\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if score < 650:\n",
    "            recommendations.append(\"‚ö†Ô∏è Score bajo requiere mejoras urgentes\")\n",
    "            \n",
    "            # Analizar factores negativos\n",
    "            negative_factors = factors[factors['Impacto'] > 0].head(3)\n",
    "            for _, factor in negative_factors.iterrows():\n",
    "                if 'missed_payments' in factor['Factor']:\n",
    "                    recommendations.append(\"üìå Reducir pagos atrasados\")\n",
    "                elif 'debt_to_income' in factor['Factor']:\n",
    "                    recommendations.append(\"üìå Reducir ratio deuda/ingreso\")\n",
    "                elif 'credit_utilization' in factor['Factor']:\n",
    "                    recommendations.append(\"üìå Usar menos del 30% del cr√©dito disponible\")\n",
    "        else:\n",
    "            recommendations.append(\"‚úÖ Score aceptable, mantener buen comportamiento\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def plot_report(self, report):\n",
    "        \"\"\"Visualizar reporte de cr√©dito\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # 1. Score Gauge\n",
    "        ax = axes[0, 0]\n",
    "        ax.axis('equal')\n",
    "        \n",
    "        # Crear gauge\n",
    "        theta = np.linspace(np.pi, 0, 100)\n",
    "        r_inner = 0.7\n",
    "        r_outer = 1.0\n",
    "        \n",
    "        # Colores por segmento\n",
    "        colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n",
    "        bounds = [300, 600, 650, 700, 750, 850]\n",
    "        \n",
    "        for i in range(len(colors)):\n",
    "            start_angle = np.pi - (bounds[i] - 300) / 550 * np.pi\n",
    "            end_angle = np.pi - (bounds[i+1] - 300) / 550 * np.pi\n",
    "            theta_seg = np.linspace(start_angle, end_angle, 20)\n",
    "            \n",
    "            x_inner = r_inner * np.cos(theta_seg)\n",
    "            y_inner = r_inner * np.sin(theta_seg)\n",
    "            x_outer = r_outer * np.cos(theta_seg)\n",
    "            y_outer = r_outer * np.sin(theta_seg)\n",
    "            \n",
    "            verts = list(zip(x_outer, y_outer)) + list(zip(x_inner[::-1], y_inner[::-1]))\n",
    "            poly = plt.Polygon(verts, facecolor=colors[i], edgecolor='white', linewidth=2)\n",
    "            ax.add_patch(poly)\n",
    "        \n",
    "        # Aguja del score\n",
    "        score_angle = np.pi - (report['credit_score'] - 300) / 550 * np.pi\n",
    "        ax.arrow(0, 0, 0.9 * np.cos(score_angle), 0.9 * np.sin(score_angle),\n",
    "                head_width=0.05, head_length=0.05, fc='black', ec='black', linewidth=2)\n",
    "        \n",
    "        # Centro\n",
    "        circle = plt.Circle((0, 0), 0.05, color='black')\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        # Texto\n",
    "        ax.text(0, -0.3, f\"{report['credit_score']}\", \n",
    "               fontsize=36, fontweight='bold', ha='center')\n",
    "        ax.text(0, -0.45, report['risk_category'], \n",
    "               fontsize=16, ha='center', color=report['risk_color'])\n",
    "        \n",
    "        ax.set_xlim(-1.2, 1.2)\n",
    "        ax.set_ylim(-0.6, 1.2)\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Credit Score', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 2. Probabilidad de Default\n",
    "        ax = axes[0, 1]\n",
    "        prob = report['default_probability']\n",
    "        \n",
    "        # Barra de probabilidad\n",
    "        ax.barh(['Probabilidad\\nde Default'], [prob], color='red', alpha=0.7)\n",
    "        ax.barh(['Probabilidad\\nde No Default'], [1-prob], left=[prob], color='green', alpha=0.7)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_xlabel('Probabilidad')\n",
    "        ax.set_title('Riesgo de Default', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Texto con porcentaje\n",
    "        ax.text(prob/2, 0, f'{prob:.1%}', ha='center', va='center', \n",
    "               fontsize=12, fontweight='bold', color='white')\n",
    "        ax.text(prob + (1-prob)/2, 0, f'{1-prob:.1%}', ha='center', va='center',\n",
    "               fontsize=12, fontweight='bold', color='white')\n",
    "        \n",
    "        # 3. Factores principales\n",
    "        ax = axes[1, 0]\n",
    "        factors = report['main_factors']\n",
    "        colors_factors = ['red' if x > 0 else 'green' for x in factors['Impacto']]\n",
    "        \n",
    "        ax.barh(factors['Factor'][:5], factors['Impacto'][:5], color=colors_factors)\n",
    "        ax.set_xlabel('Impacto en Score')\n",
    "        ax.set_title('Factores Principales', fontsize=14, fontweight='bold')\n",
    "        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        \n",
    "        # 4. Resumen y recomendaciones\n",
    "        ax = axes[1, 1]\n",
    "        ax.axis('off')\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "üìä RESUMEN DE CR√âDITO\n",
    "        \n",
    "üí≥ L√≠mite Aprobado: ${report['credit_limit']:,}\n",
    "\n",
    "üìà Categor√≠a de Riesgo: {report['risk_category']}\n",
    "\n",
    "üéØ Probabilidad de Default: {report['default_probability']:.1%}\n",
    "\n",
    "üìã RECOMENDACIONES:\n",
    "\"\"\"\n",
    "        for rec in report['recommendation']:\n",
    "            summary_text += f\"\\n{rec}\"\n",
    "        \n",
    "        ax.text(0.1, 0.9, summary_text, transform=ax.transAxes,\n",
    "               fontsize=11, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "        \n",
    "        plt.suptitle(\"Reporte de Scoring Crediticio\", fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Entrenar sistema\n",
    "print(\"\\nüéØ Entrenando Sistema de Scoring...\")\n",
    "\n",
    "# Split datos\n",
    "X = credit_data.drop('default', axis=1)\n",
    "y = credit_data['default']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Crear y entrenar sistema\n",
    "scoring_system = CreditScoringSystem()\n",
    "scoring_system.train(X_train, y_train)\n",
    "\n",
    "# Evaluar\n",
    "y_pred_proba = scoring_system.predict_default_probability(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nüìä Evaluaci√≥n del Sistema:\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"  ‚Ä¢ AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# Generar reporte para un cliente ejemplo\n",
    "print(\"\\nüìã Generando reporte para cliente ejemplo...\")\n",
    "sample_customer = X_test.iloc[[0]]\n",
    "report = scoring_system.generate_report(sample_customer)\n",
    "\n",
    "print(f\"\\nüéØ Resultado para Cliente:\")\n",
    "print(f\"  ‚Ä¢ Credit Score: {report['credit_score']}\")\n",
    "print(f\"  ‚Ä¢ Categor√≠a: {report['risk_category']}\")\n",
    "print(f\"  ‚Ä¢ L√≠mite de Cr√©dito: ${report['credit_limit']:,}\")\n",
    "\n",
    "# Visualizar reporte\n",
    "scoring_system.plot_report(report)\n",
    "\n",
    "# Feature importance global\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top_features = scoring_system.feature_importance.head(10)\n",
    "ax.barh(top_features['feature'], top_features['importance'], color=COLORS['primary'])\n",
    "ax.set_xlabel('Importancia (|Coeficiente|)')\n",
    "ax.set_title('Importancia Global de Features en Scoring', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéì PROYECTO COMPLETADO\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "‚úÖ Has implementado exitosamente:\n",
    "1. Regresi√≥n Lineal desde cero\n",
    "2. Gradient Descent manual\n",
    "3. Diagn√≥stico completo de modelos\n",
    "4. T√©cnicas de regularizaci√≥n\n",
    "5. Regresi√≥n Log√≠stica desde cero\n",
    "6. Manejo de clases desbalanceadas\n",
    "7. Sistema completo de scoring crediticio\n",
    "\n",
    "üöÄ Pr√≥ximos pasos sugeridos:\n",
    "- Experimentar con m√°s datasets reales\n",
    "- Implementar validaci√≥n cruzada temporal\n",
    "- A√±adir interpretabilidad (SHAP, LIME)\n",
    "- Desplegar modelo en producci√≥n\n",
    "- Monitorear drift y performance\n",
    "\n",
    "¬°Felicitaciones por completar el notebook! üéâ\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Conclusi√≥n\n",
    "\n",
    "Este notebook ha cubierto exhaustivamente los conceptos de **Regresi√≥n Lineal y Log√≠stica**, desde implementaciones manuales hasta sistemas completos de producci√≥n. Has aprendido:\n",
    "\n",
    "1. **Fundamentos matem√°ticos** y su implementaci√≥n pr√°ctica\n",
    "2. **Diagn√≥stico y evaluaci√≥n** de modelos\n",
    "3. **T√©cnicas de regularizaci√≥n** para evitar overfitting\n",
    "4. **Manejo de problemas reales** como desbalance y multicolinealidad\n",
    "5. **Construcci√≥n de sistemas completos** de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ CERTIFICADO DE COMPLETACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Fecha: {datetime.now().strftime(\"%d de %B, %Y\")}\n",
    "\n",
    "Felicitaciones por completar exitosamente el notebook\n",
    "\"Regresi√≥n Lineal y Log√≠stica: Gu√≠a Completa\"\n",
    "\n",
    "Has demostrado dominio en:\n",
    "‚úì Implementaci√≥n de algoritmos desde cero\n",
    "‚úì Uso de librer√≠as de Machine Learning\n",
    "‚úì Diagn√≥stico y optimizaci√≥n de modelos\n",
    "‚úì Resoluci√≥n de problemas pr√°cticos\n",
    "‚úì Construcci√≥n de sistemas de producci√≥n\n",
    "\n",
    "¬°Contin√∫a tu viaje en Machine Learning! üöÄ\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fin del Notebook**\n",
    "\n",
    "*√öltima actualizaci√≥n: 2025*  \n",
    "*Autor: Adaptado y mejorado para el curso de Machine Learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
